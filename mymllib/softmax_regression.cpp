#include "softmax_regression.h"
#include<cmath>
namespace myml
{
	using namespace matrix_operate;
	softmax_regression::softmax_regression(size_t feature_count, size_t label_count)
	{
		_theta.resize(label_count, feature_count + 1);
		_theta.fill(1);
		_pre_theta = _theta;
		_cur_error = rand();
		_last_error = rand();
	}

	void softmax_regression::import_data(const matrix<feature_type>& feature_matrix, const matrix<label_type>& label_matrix)
	{

	}
	void softmax_regression::sgd(matrix<feature_type>& feature_matrix, matrix<label_type>& label_matrix)
	{
		for (size_t row_i = 0; row_i < feature_matrix.row_size(); ++row_i)
		{
			matrix<param_type> err_matrix = error_matrix(feature_matrix.row(row_i), label_matrix.row(row_i));
			_pre_theta = _theta;
			_theta += (err_matrix *= _learn_rate);
		}
	}

	void softmax_regression::batch_gd(matrix<feature_type>& feature_matrix, matrix<label_type>& label_matrix)
	{		
		matrix<feature_type> err_matrix = error_matrix(feature_matrix, label_matrix);
		err_matrix /= feature_type(feature_matrix.row_size());
		_cur_error = norm_f(err_matrix);
		
		_pre_theta = _theta;
		_theta += ((err_matrix) *= _learn_rate);

		_last_error = _cur_error;
		
	}

	matrix<label_type> softmax_regression::predict(matrix<feature_type>& feature_matrix)
	{
		matrix<label_type> result(feature_matrix.row_size(), 1);
		for (size_t row_i = 0; row_i < feature_matrix.row_size(); ++row_i)
		{
			matrix<feature_type> row = matrix<feature_type>(1, feature_matrix.col_size() + 1);
			row.col(feature_matrix.col_size()).fill(1);
			row.cols(0, feature_matrix.col_size() - 1) = feature_matrix.row(row_i);
			row.transpose();
			matrix<feature_type> predict_result = (_theta * row);

			param_type max = -LDBL_MAX;
			label_type max_i = 0;
			for (size_t row_i = 0; row_i < predict_result.row_size(); ++row_i)
			{
				if (predict_result.row(row_i) > max)
				{
					max = predict_result.row(row_i);
					max_i = row_i;
				}
			}
			label_type(result.row(row_i)) = max_i;
		}
		return result;
	}




	param_type softmax_regression::accuracy(matrix<feature_type>& feature_matrix, matrix<label_type>& label_matrix)
	{
		matrix<label_type> pr = predict(feature_matrix);
		size_t total = pr.row_size();
		size_t correct = 0;
		for (size_t row_i = 0; row_i < feature_matrix.row_size(); ++row_i)
		{
			if (pr.row(row_i) == label_matrix.row(row_i))
				correct++;
		}
		return (correct + .0) / total;
	}

	matrix<param_type> softmax_regression::error_matrix(matrix<feature_type>& feature_matrix, matrix<label_type>& label_matrix)
	{
		/*误差矩阵大小与_theta大小相同*/
		matrix<feature_type> sum_error(_theta.row_size(), _theta.col_size());
		/*累加每个特征向量的误差*/
#pragma omp parallel for
		for (size_t row_i = 0; row_i < feature_matrix.row_size(); ++row_i)
		{
			/*临时特征向量，增加特征向量的维数，计算常数项*/
			matrix<feature_type> row = matrix<feature_type>(1, feature_matrix.col_size() + 1);
			/*临时特征向量的最后一维与常数项系数相乘，设置为1*/
			row.col(feature_matrix.col_size()).fill(1);
			/*临时特征向量赋值*/
			row.cols(0, feature_matrix.col_size() - 1) = feature_matrix.row(row_i);
			/*转置临时向量*/
			row.transpose();

			/*
			特征向量的预测结果predict_result，的行数与类标数相同
			第i行即该特征向量为第i类的概率
			*/
			/*系数矩阵乘以行的线性结果*/
			matrix<feature_type> predict_result = (_theta * row);
			/*softmax函数处理*/
			exp(predict_result);
			predict_result /= sum(predict_result);
			/*准确结果为，特征向量的真正类标的那一行应该为1，其他行应该为0
			使用准确结果减去真正结果
			*/
			for (size_t theta_i = 0; theta_i < predict_result.row_size(); ++theta_i)
			{
				//准确结果减去预测结果为误差
				feature_type(predict_result.row(theta_i)) = (theta_i == label_matrix.row(row_i) ? 1 - predict_result.row(theta_i) : 0 - predict_result.row(theta_i));
			}
			/*预测结果的每一行误差，乘以倒数中的特征向量项*/
			sum_error += predict_result * row.t();
		}
		return sum_error;
	}

}